================================================================================
                    MLflow S3 Storage - Implementation Summary
================================================================================

OVERVIEW
--------
Your BasketWorld project now supports storing MLflow experiments and artifacts
in your personal S3 bucket, while maintaining full backward compatibility with
local storage.

WHAT WAS ADDED
--------------
1. Configuration Module (basketworld/utils/mlflow_config.py)
   - Automatic detection of S3 vs local storage
   - Validation of dependencies and credentials
   - Centralized configuration management

2. Updated Dependencies (requirements.txt)
   - Added boto3==1.37.3 for AWS S3 support

3. Updated All Scripts
   - train/train.py
   - app/backend/main.py
   - All analytics scripts (elo_evolution, evaluate, heatmap, etc.)
   - scripts/cleanup_mlflow_deleted_runs.py

4. Comprehensive Documentation
   - docs/mlflow_s3_setup.md (full guide)
   - docs/mlflow_s3_quickstart.md (quick reference)
   - docs/MLFLOW_S3_IMPLEMENTATION.md (technical details)
   - Updated README.md with S3 section

5. Test Script (scripts/test_mlflow_s3.py)
   - Validates S3 configuration
   - Tests connectivity and permissions

QUICK START (Local Storage - No Changes Needed)
------------------------------------------------
Everything works exactly as before:

  mlflow ui
  python train/train.py --mlflow-experiment-name my-experiment

QUICK START (S3 Storage - 5 Minutes)
-------------------------------------
1. Install boto3 (already in requirements.txt):
   pip install boto3

2. Set environment variables:
   export AWS_ACCESS_KEY_ID="your-access-key"
   export AWS_SECRET_ACCESS_KEY="your-secret-key"
   export AWS_DEFAULT_REGION="us-east-1"
   export MLFLOW_ARTIFACT_ROOT="s3://your-bucket/mlflow-artifacts"

3. Start MLflow server with S3:
   mlflow server \
     --backend-store-uri sqlite:///mlflow.db \
     --default-artifact-root s3://your-bucket/mlflow-artifacts \
     --port 5000

4. Run training (automatically uses S3):
   python train/train.py --mlflow-experiment-name my-experiment

   You'll see:
   MLflow Configuration:
     Tracking URI: http://localhost:5000
     Storage Type: S3 (Remote)
     Artifact Root: s3://your-bucket/mlflow-artifacts

TESTING YOUR SETUP
------------------
Run the test script to verify everything is configured correctly:

  python scripts/test_mlflow_s3.py

This will check:
  ✓ Environment variables
  ✓ boto3 installation
  ✓ AWS credentials
  ✓ S3 connectivity
  ✓ MLflow configuration
  ✓ MLflow server connectivity

SWITCHING BETWEEN LOCAL AND S3
-------------------------------
Local Storage:
  unset MLFLOW_ARTIFACT_ROOT
  mlflow ui

S3 Storage:
  export MLFLOW_ARTIFACT_ROOT="s3://your-bucket/mlflow-artifacts"
  mlflow server --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root s3://your-bucket/mlflow-artifacts

PERSISTENT CONFIGURATION
-------------------------
Add to ~/.bashrc or ~/.zshrc to avoid setting every time:

  export AWS_ACCESS_KEY_ID="your-key"
  export AWS_SECRET_ACCESS_KEY="your-secret"
  export AWS_DEFAULT_REGION="us-east-1"
  export MLFLOW_ARTIFACT_ROOT="s3://your-bucket/mlflow-artifacts"

Then: source ~/.bashrc

KEY FEATURES
------------
✓ Automatic detection - no code changes needed
✓ Backward compatible - local storage still works
✓ All scripts updated - training, backend, analytics
✓ Comprehensive documentation
✓ Test script for validation
✓ Secure - credentials never committed to git (.env is in .gitignore)

REQUIRED IAM PERMISSIONS
------------------------
Your AWS IAM user needs these S3 permissions:
  - s3:PutObject
  - s3:GetObject
  - s3:DeleteObject
  - s3:ListBucket

See docs/mlflow_s3_setup.md for a complete IAM policy example.

WHAT GETS STORED IN S3
-----------------------
✓ Model checkpoints (.zip files)
✓ Plots and visualizations
✓ Custom artifacts logged during training

Note: Experiment metadata (run IDs, parameters, metrics) stays in the
      backend store (SQLite/PostgreSQL), not in S3.

DOCUMENTATION
-------------
Full Guide:        docs/mlflow_s3_setup.md
Quick Reference:   docs/mlflow_s3_quickstart.md
Technical Details: docs/MLFLOW_S3_IMPLEMENTATION.md
README Section:    README.md (search for "Remote Storage")

TROUBLESHOOTING
---------------
Problem: "boto3 is required"
Solution: pip install boto3

Problem: "AWS_ACCESS_KEY_ID must be set"
Solution: Export AWS credentials as environment variables

Problem: "NoSuchBucket"
Solution: Check bucket name, region, and IAM permissions

Problem: Artifacts not uploading to S3
Solution: Ensure MLflow server was started with --default-artifact-root s3://...

See docs/mlflow_s3_setup.md for detailed troubleshooting.

COST CONSIDERATIONS
-------------------
Approximate costs for 100 GB of artifacts:
  - S3 Standard: ~$2.30/month
  - S3 Infrequent Access: ~$1.25/month (after 30 days)
  - Data Transfer: Out to internet ~$9/GB (free within AWS)

Use lifecycle policies to automatically move old artifacts to cheaper tiers.

SECURITY BEST PRACTICES
------------------------
✓ Never commit credentials (already in .gitignore)
✓ Use IAM roles on EC2 instances
✓ Enable S3 server-side encryption
✓ Enable S3 versioning
✓ Use least-privilege IAM policies
✓ Enable S3 access logging

NEXT STEPS
----------
1. Read docs/mlflow_s3_quickstart.md for setup instructions
2. Create an S3 bucket in AWS Console
3. Create an IAM user with S3 permissions
4. Set environment variables
5. Run python scripts/test_mlflow_s3.py to verify setup
6. Start training with S3 storage!

SUMMARY
-------
Everything is ready to go! The implementation is:
  • Automatic - detects S3 configuration from environment variables
  • Transparent - no code changes needed in your workflow
  • Tested - includes comprehensive test script
  • Documented - full guides and quick references
  • Secure - credentials managed via environment variables

You can start using S3 storage immediately, or continue using local storage.
Both work seamlessly without any code changes.

================================================================================

